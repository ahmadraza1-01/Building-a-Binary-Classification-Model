# Binary Classification and Dataset Combination for Improved Performance  

This repository includes the implementation of binary classification models across diverse datasets and their subsequent combination to enhance model performance. The project explores various machine learning techniques for binary classification and demonstrates the utility of integrating multiple datasets for improved accuracy.  

## Features  
### Task 1: Binary Classification Models  
- **Emoticons as Features Dataset**:  
  - Processed emoji-based features using one-hot encoding.  
  - Trained Logistic Regression and Stochastic Gradient Descent (SGD) models, including feature selection for noise reduction.  

- **Deep Features Dataset**:  
  - Leveraged embeddings generated by pre-trained models as input features.  
  - Evaluated models such as Logistic Regression, KNN, Support Vector Machine (SVM), and Random Forest.  

- **Text Sequence Dataset**:  
  - Explored character-sequence classification using XGBoost (with and without hyperparameters) and LSTM models.  

### Task 2: Combining Datasets for Enhanced Performance  
- **Feature Combination**: Merged emoticon features, deep features, and text sequence features into a single feature vector (831 dimensions).  
- **Model Training**: Trained Random Forest on the combined dataset with hyperparameter optimization for regularization.  

## Methodology  
1. **Feature Preprocessing**:  
   - Applied one-hot encoding, flattening, and scaling techniques across datasets.  
   - Removed noisy or irrelevant features to enhance performance.  

2. **Model Development**:  
   - Conducted experiments with Logistic Regression, SVM, SGD, XGBoost, Random Forest, and LSTM.  
   - Used GridSearchCV and RandomizedSearchCV for hyperparameter tuning.  

3. **Dataset Combination**:  
   - Merged datasets horizontally to leverage complementary information.  
   - Ensured proper alignment across datasets for accurate feature integration.  

4. **Experimental Process**:  
   - Evaluated models using varying training data fractions (20% to 100%).  
   - Recorded metrics such as accuracy, confusion matrices, and classification reports.  

## Results  
- **Best Individual Models**:  
  - Emoticon Dataset: SGD with feature selection (96.5% accuracy).  
  - Deep Features Dataset: Random Forest (98% accuracy).  
  - Text Sequence Dataset: XGBoost without hyperparameters (87.2% accuracy).  

- **Combined Dataset**:  
  - Random Forest achieved 98.4% validation accuracy with the unified dataset, outperforming individual models.  

## Strengths  
- Effective preprocessing and feature engineering across datasets.  
- Improved performance through dataset integration and regularization.  
- Demonstrated scalability across varied data representations.  

## Limitations  
- Feature noise in the Emoticon dataset impacts certain models.  
- Overfitting observed in XGBoost with hyperparameter tuning.  

## Conclusion  
This project highlights the effectiveness of dataset combination and advanced machine learning techniques for binary classification tasks. The Random Forest model on the combined dataset proved to be the most robust, achieving the highest validation accuracy.  

---  
